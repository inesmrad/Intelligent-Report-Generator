# -*- coding: utf-8 -*-
"""CNN DL Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q5zWSPTtQ7Ac9ccEMhCVIVaI_zQnlLF4
"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("/content/merged_electronics_dataset.csv", on_bad_lines='skip')

print(df.head())
print(df.tail())
print(df.shape)

print(df.columns)

# Create 'brand' column by taking the first word of 'name'
df['brand'] = df['name'].apply(lambda x: str(x).split()[0])

# Check
print(df[['name', 'brand']].head())

print(df.columns)

df_clean = df.dropna(subset=['image', 'brand','name']).reset_index(drop=True)
print(df_clean.shape)

df_clean = df_clean[['brand', 'image','name']]

#download images
import os
import requests
from tqdm import tqdm

# Folder to save images
image_dir = '/content/images'
os.makedirs(image_dir, exist_ok=True)

# Download images
df_clean['image_path'] = None  # new column for local image path

for idx, row in tqdm(df_clean.iterrows(), total=df_clean.shape[0]):
    url = row['image']
    if pd.isna(url):
        continue
    try:
        response = requests.get(url, timeout=5)
        ext = url.split('.')[-1].split('?')[0]  # get jpg/png extension
        file_path = os.path.join(image_dir, f"{idx}.{ext}")
        with open(file_path, 'wb') as f:
            f.write(response.content)
        df_clean.at[idx, 'image_path'] = file_path
    except:
        pass

# Keep only rows where download succeeded
df_clean = df_clean.dropna(subset=['image_path']).reset_index(drop=True)
print(df_clean.shape)

import os
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Correct imports for loading and converting images
from tensorflow.keras.utils import load_img, img_to_array

# Encode brands
le_brand = LabelEncoder()
df_clean['brand_encoded'] = le_brand.fit_transform(df_clean['brand'])
num_brands = df_clean['brand_encoded'].nunique()
print("Number of brands:", num_brands)

df_clean['image_path'] = df_clean.index.map(lambda i: f"/content/images/{i}.jpg")

# Convert brand_encoded to strings
df_clean['brand_encoded_str'] = df_clean['brand_encoded'].astype(str)

from PIL import Image

valid_paths = []
for path in df_clean['image_path']:
    try:
        img = Image.open(path)
        img.verify()  # checks if image can be opened
        valid_paths.append(path)
    except:
        pass

# Keep only rows with valid images
df_clean = df_clean[df_clean['image_path'].isin(valid_paths)].reset_index(drop=True)
print("Number of valid images:", len(df_clean))

#prepare image data generators
# Data augmentation for brand classification
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,  # 80% train, 20% validation
    horizontal_flip=True,
    rotation_range=20,
    zoom_range=0.2
)

train_gen = datagen.flow_from_dataframe(
    dataframe=df_clean,
    x_col='image_path',
    y_col='brand_encoded_str',  # use string labels
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',   # categorical works with string labels
    subset='training'
)

val_gen = datagen.flow_from_dataframe(
    dataframe=df_clean,
    x_col='image_path',
    y_col='brand_encoded_str',
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

#CNN will learn to classify brands, not individual products yet.

#build the cnn transfer learning
# Pretrained CNN as feature extractor
base_model = EfficientNetB0(include_top=False, input_shape=(224,224,3), weights='imagenet')
base_model.trainable = False  # freeze pretrained layers

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
output = Dense(num_brands, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# After filtering invalid images
num_brands = df_clean['brand_encoded_str'].nunique()
print("Number of brands after filtering:", num_brands)

# Rebuild the model output layer
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

base_model = EfficientNetB0(include_top=False, input_shape=(224,224,3), weights='imagenet')
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
output = Dense(num_brands, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=output)

# Compile
model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=10,
    steps_per_epoch=train_gen.samples // train_gen.batch_size,
    validation_steps=val_gen.samples // val_gen.batch_size
)

#After training, the CNN can predict the brand from a new product image

"""What you need to save

For your CNN brand prediction, there are two things you need to save:

The trained CNN model → this contains the neural network itself and its learned weights.

It is used to predict the brand from an image.

The LabelEncoder (le) → this maps brand names ↔ integers.

Your CNN outputs integers (like 0, 1, 2…), but you need the actual brand name for your project.

Saving le lets you convert the model output back to a human-readable brand.
"""

# Save the entire CNN model (architecture + weights)
model.save('/content/brand_cnn_model.h5')

import pickle

# Save LabelEncoder
with open('/content/brand_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)

"""We built a CNN for brand classification.

We used EfficientNetB0 as the base, which is pretrained on ImageNet, so the convolutional layers are not trained from scratch ,they provide learned feature extraction.

We added a GlobalAveragePooling2D layer, a Dropout layer, and a Dense layer with softmax on top to classify into your num_brands classes.

So, we designed the classification head (pooling, dropout, dense) from scratch, but the core CNN features come from EfficientNetB0, which is pretrained.

✅ In short: the brand-prediction CNN head is our design, but the convolutional backbone is transfer learning, not from scratch.
"""

import os
from PIL import Image

valid_files = []
for filename in os.listdir('/content/images'):
    path = os.path.join('/content/images', filename)
    try:
        img = Image.open(path)
        img.verify()  # Check if image is readable
        valid_files.append(path)
    except:
        print(f"Removing corrupted image: {path}")
        os.remove(path)  # Delete corrupted file immediately

df_clean = df_clean[df_clean['image_path'].isin(valid_files)].reset_index(drop=True)
print("Remaining valid images:", df_clean.shape[0])

df_clean = df_clean[['name', 'image']].copy()
print(df_clean.head())

import os

# Keep only rows with actual image files
df_clean = df_clean[df_clean['image_path'].apply(os.path.isfile)].reset_index(drop=True)

print("Remaining images:", df_clean.shape[0])

#Step 5: Extract image embeddings for similarity search
#We will create feature vectors for all images to compare images within the same brand.

from tensorflow.keras.preprocessing import image
import numpy as np
from tqdm import tqdm
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D

embedding_model = Model(inputs=base_model.input, outputs=GlobalAveragePooling2D()(base_model.output))
embeddings = {}

for idx, row in tqdm(df_clean.iterrows(), total=df_clean.shape[0]):
    img_path = row['image_path']
    img = image.load_img(img_path, target_size=(224,224))
    img_array = image.img_to_array(img)/255.0
    img_array = np.expand_dims(img_array, axis=0)
    emb = embedding_model.predict(img_array, verbose=0)
    embeddings[row['name']] = emb.flatten()

#Predict brand + find most similar product

def predict_product(img_path, top_k=1):
    # 1️⃣ Predict brand
    img = image.load_img(img_path, target_size=(224,224))
    x = image.img_to_array(img)/255.0
    x = np.expand_dims(x, axis=0)

    brand_pred = model.predict(x)#brand_pred: array of size (1, num_brands) → probability for each brand.
    brand_idx = np.argmax(brand_pred)#brand_idx: integer → index of predicted brand.
    brand_name = le_brand.inverse_transform([brand_idx])[0] #brand_name: string → the predicted brand.

    # 2️⃣ Find embeddings of products with the same brand
    brand_products = df_clean[df_clean['brand'] == brand_name]['name'].tolist()
    brand_embeddings = np.array([embeddings[name] for name in brand_products])

    # 3️⃣ Compute similarity with input image embedding
    img_emb = embedding_model.predict(x).flatten().reshape(1, -1)
    similarities = cosine_similarity(img_emb, brand_embeddings).flatten()

    # 4️⃣ Pick most similar product
    best_idx = np.argmax(similarities)
    predicted_product = brand_products[best_idx]

    return brand_name, predicted_product

# Extract brand as first word of the product name
df['brand'] = df['name'].apply(lambda x: str(x).split()[0])

# Keep only the columns you need
df_clean = df.dropna(subset=['image', 'name']).reset_index(drop=True)

# Add the brand column to df_clean
df_clean['brand'] = df['brand']

brand, product = predict_product('/content/images/0.jpg')
print("Predicted brand:", brand)
print("Predicted product:", product)

model.save('brand_classifier.h5')

# embeddings = {product_name: embedding_vector}
import pickle

with open('embeddings.pkl', 'wb') as f:
    pickle.dump(embeddings, f)

import pickle

with open('le_brand.pkl', 'wb') as f:
    pickle.dump(le_brand, f)

"""Input image
     │
     ▼
[ CNN predicts brand ]
     │
     ▼
Filter dataset to same brand
     │
     ▼
Compute cosine similarity with embeddings
     │
     ▼
Return most visually similar product

1️⃣ What the hybrid approach is

Your goal:

Give the system a product image → it tells you exactly which product it is.

The problem with a direct approach:

If you try to classify all products directly with a CNN, it has to learn thousands of unique classes (e.g., 5,000+ product names).

This is very hard, because many products look very similar. Accuracy drops a lot.

Hybrid solution:
It uses two stages:

2️⃣ Stage 1: CNN predicts the brand

You train a CNN to classify brands only, not individual products.

Example: Given an image of a phone, the CNN predicts Samsung or OnePlus.

Why this works:

There are fewer brands than products (e.g., 50 brands vs. 5,000 products).

CNN focuses on brand-level visual features like logo, style, or design patterns.

✅ This reduces the complexity and makes the model more accurate.

3️⃣ Stage 2: Image similarity search for exact product

After knowing the brand, we don’t need to compare the input image with all products—only products of that brand.

For each product image, we extract a feature vector (embedding) using the CNN (without the classification layer) and GlobalAveragePooling2D.

These embeddings are numerical representations of images.

Images that look visually similar have closer embeddings in the vector space.

We compute similarity between the input image embedding and all embeddings of that brand using cosine similarity.

The product with the highest similarity score is returned as the predicted product.

✅ This allows you to identify the exact product, even if the CNN only learned brand-level features.

4️⃣ Why this is a hybrid approach

Hybrid = combines two methods:

CNN classification → predict the brand (coarse-level classification)

Image similarity search → pick the exact product (fine-grained recognition)

This approach reduces the number of classes the CNN must handle and still allows you to distinguish individual products.

5️⃣ Why it solves your problem

Direct classification on thousands of product names → too hard, low accuracy.

Hybrid method:

CNN handles the easy task (brand-level classification).

Similarity search handles the hard task (distinguishing individual products visually).

Result → more accurate and scalable solution for product recognition.
"""